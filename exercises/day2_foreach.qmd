---
title: "day2_foreach_demo"
author: "Heli Juottonen (CSC)"
format: html
editor: visual
---

## Getting started with parallel R: `foreach`

Checking how many cores there are available in the session:

```{r}
parallelly::availableCores()
```

**Important:** do not use parallel::detectCores() on Puhti - it always returns the maximum number of cores in a node (40), no matter how many have been reserved. This causes problems!

```{r}
# do not use in scripts or R jobs on Puhti - this gives the wrong number of cores!
parallel::detectCores()
```

Let's start from a simple for loop and check how long it runs:

```{r}
library(tictoc)

tic()
for (i in 1:3) {
  sqrt(i)
  Sys.sleep(5) # added to make this example script run longer
}
toc()
```

Same example as above but this time using the package `foreach` (still sequential, not parallel):

```{r}
library(foreach)

tic()
foreach(i = 1:3, .combine = 'c') %do% {
  sqrt(i)
  Sys.sleep(5)
}
toc()
```

Running the same thing **in parallel** taking advantage of **3 cores**:

```{r}
#library(doParallel)
#registerDoParallel(cores = 3) #registering a backend for foreach

tic()
foreach(i = 1:3, .combine = 'c') %dopar% {
  sqrt(i)
  Sys.sleep(5)
}
toc()

# unregistering the backend by changing back to sequential:
# registerDoSEQ()
```

What would happen without the sleep step (= a very short run)? Why?

```{r}
# sequential
tic()
foreach(i = 1:3, .combine = 'c') %do% {
  sqrt(i)
}
toc()
```

```{r}
# parallel
tic()
doParallel::registerDoParallel(cores = 3)
foreach(i = 1:3, .combine = 'c') %dopar% {
  sqrt(i)
}
toc()
```

## Exercise on `foreach`

1.  In the course project folder on Puhti `/scratch/project_2011190/shared_data` there is a folder `communities` that contains three .csv files. Copy this folder to your personal folder under `/scratch/project_2011190/personal`.

2.  Start an RStudio session on the Puhti web interface (www.puhti.csc.fi) using the following resources:

    project: project_2011190

    reservation: high_perf_r_fri

    number of CPU cores: 4

    memory: 2 GB (default)

    local disk: 32 GB (default)

    R version: 4.4.0 (default)

    time: 4:00:00 (default)

3.  First, check how long running the following code snippet takes (use one of the timing approaches introduced on day 1).

    Note: change the file path in the first command to your folder where you copied the `communities` folder

    ``` r
    # creating a list of .csv files in a folder

    comm_csv_list <- list.files(path = "/scratch/project_2011190/personal/xxxxxx/communities/", pattern = ".csv", full.names = TRUE) 

    # this for loop goes through the .csv files in the list and carries out the same operations on each of them (reads in the csv file, calculates a distance matrix and saves the matrix as a .dist file)

    for (comm_csv in comm_csv_list) {
      comm <- read.csv2(comm_csv, header = T, row.names = 1)
      dist <- vegan::vegdist(t(comm), method = "bray")
      dist <- as.matrix(dist)
      Sys.sleep(5) # added to extend the running time of the small example
      filename = gsub(".csv", ".dist", comm_csv)
      write.table(dist, filename)
    }
    ```

4.  Then, let's change the for loop into a parallel approach using `foreach`.

Check the example above for a reminder on what needed to be changed in the for loop.

Hints: Which packages do you need to load? How can you tell `foreach` how many cores to use? What needs to be added before the first curly bracket of the for loop?

What happens to the running time compared to the sequential approach above?

```{r}
# solution will appear here later

# comm_csv_list <- list.files(path = "/scratch/project_2011190/personal/helijuot/communities/", pattern = ".csv", full.names = TRUE) 
# 
# 
# for (comm_csv in comm_csv_list) {
#   comm <- read.csv2(comm_csv, header = T, row.names = 1)
#   dist <- vegan::vegdist(t(comm), method = "bray")
#   dist <- as.matrix(dist)
#   Sys.sleep(5) # added to extend the running time of the small example
#   filename = gsub(".csv", ".dist", comm_csv)
#   write.table(dist, filename)
#}
```
